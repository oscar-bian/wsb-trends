{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from multiprocessing import Process, Pipe\n",
    "from praw.models import MoreComments\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/oscar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/oscar/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['stopwords', 'words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit API access\n",
    "Once you create a Reddit app in the dev portal\n",
    "retrieve the following params from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_ID = 'ACqyZR3hYsXzfA'\n",
    "APP_SECRET = 'xehsNEoOe_EH9DT-JwxekUDXLKxOIA'\n",
    "USER_AGENT = 'my little scrapper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRAW: Python Reddit API Wrapper\n",
    "Does all the hard work\n",
    "https://praw.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.1.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "     client_id=APP_ID,\n",
    "     client_secret=APP_SECRET,\n",
    "     user_agent=USER_AGENT,\n",
    "     username=\"obiflow\",\n",
    "     password=\"Jmv94johgier!*8UwBC6kUBe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit(\"wallstreetbets\")\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short loop\n",
    "\n",
    "only traverse the comment tree root\n",
    "only retrieves top level comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing comments for What Are Your Moves Tomorrow, June 03, 2021\n",
      "Parsing comments for Most Anticipated Earnings Releases for the week beginning May 31st, 2021\n",
      "Parsing comments for I put 750 dollars in I have never had this much I know my dads looking down smiling thankyou apes\n",
      "Parsing comments for To those who called me a r*tard for going all in on AMC 🤡🖕🏻\n",
      "Parsing comments for We can’t stop... They can’t stop us, although they try... we’re true soldiers, we don’t die... We keep rolling... na-nah-nah-nah-nah!!!\n"
     ]
    }
   ],
   "source": [
    "hot_submissions = {}\n",
    "\n",
    "for submission in subreddit.hot(limit=5):\n",
    "    print('Parsing comments for {}'.format(submission.title))\n",
    "    author_name = submission.author.name\n",
    "    author_redditor_instance = reddit.redditor(author_name)\n",
    "    hot_submissions[submission.title] = {\n",
    "    'comments': [comment for comment in submission.comments if not isinstance(comment, MoreComments)],\n",
    "    'meta': {\n",
    "        'score': submission.score,  # Output: the submission's score\n",
    "        'submission_id': submission.id,     # Output: the submission's ID\n",
    "        'submission_url': submission.url,   # Output: the URL the submission points to\n",
    "        'author': {\n",
    "            'name': author_name,\n",
    "            'comment_karma': author_redditor_instance.comment_karma\n",
    "        }\n",
    "    }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long loop\n",
    "\n",
    "Go through all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hot_submissions = {}\n",
    "\n",
    "for submission in subreddit.hot(limit=25):\n",
    "    print('Parsing comments for {}'.format(submission.title))\n",
    "    author_name = submission.author.name\n",
    "    #author_redditor_instance = reddit.redditor(author_name)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    hot_submissions[submission.title] = {\n",
    "    'comments': submission.comments.list(),\n",
    "    'meta': {\n",
    "        'score': submission.score,  # Output: the submission's score\n",
    "        'submission_id': submission.id,     # Output: the submission's ID\n",
    "        'submission_url': submission.url,   # Output: the URL the submission points to\n",
    "        'author': {\n",
    "            'name': author_name,\n",
    "            'comment_karma': author_redditor_instance.comment_karma\n",
    "        }\n",
    "    }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_comment_karma(author_name):\n",
    "    redditor = reddit.redditor(author_name)\n",
    "    comment_karma = redditor.comment_karma if redditor.comment_karma else None\n",
    "    return comment_karma\n",
    "\n",
    "def parse_comment(comment, title, score):\n",
    "    if hasattr(comment, 'body'):\n",
    "        author_name = comment.author.name if comment.author else None\n",
    "        num_of_replies = 0\n",
    "        if (len(comment.replies._comments) > 0):\n",
    "            num_of_replies = comment.replies._comments[0].count if hasattr(comment.replies._comments[0], 'count') else 0\n",
    "        parsed_comment = {\n",
    "            'comment_id': comment.id,\n",
    "            'thread': title,\n",
    "            'body': comment.body,\n",
    "            'author': author_name,\n",
    "            'ups': comment.ups,\n",
    "            'downs': comment.downs,\n",
    "            'awards_count': comment.total_awards_received,\n",
    "            'comment_score': comment.score,\n",
    "            'created_at': comment.created,\n",
    "            'num_of_replies': num_of_replies,\n",
    "            'submission_score': comment.score\n",
    "        }\n",
    "        return parsed_comment\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed_comments = []\n",
    "\n",
    "for submission_title, submission_content in hot_submissions.items():\n",
    "    comments = submission_content['comments']\n",
    "    for comment in comments:\n",
    "        parsed_comment = parse_comment(comment, submission_title, submission_content['meta']['score'])\n",
    "        if parse_comment:\n",
    "            all_parsed_comments.append(parsed_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Are Your Moves Tomorrow, June 03, 2021\n",
      "Most Anticipated Earnings Releases for the week beginning May 31st, 2021\n",
      "I put 750 dollars in I have never had this much I know my dads looking down smiling thankyou apes\n",
      "To those who called me a r*tard for going all in on AMC 🤡🖕🏻\n",
      "We can’t stop... They can’t stop us, although they try... we’re true soldiers, we don’t die... We keep rolling... na-nah-nah-nah-nah!!!\n"
     ]
    }
   ],
   "source": [
    "# first version, assumes you do not know the stocks by name\n",
    "# uses regex to filter comment for exaclty three capital letters in a row\n",
    "# with a space or a dollar sign before and a space after\n",
    "# returns too many common words, not usable right now\n",
    "# TODO look into exclusion with a word library\n",
    "\n",
    "stock_name_re = '( \\$[A-Z]{3,5} | [A-Z]{3,5} )'\n",
    "\n",
    "\n",
    "bullish_words = ['hold', 'buy', 'buying', 'holding', 'yolo', 'long', 'bull', 'up', 'keep', 'moon', 'go', 'launch']\n",
    "bearish_words = ['sell', 'short', 'shorting', 'selling', 'bear', 'down', 'crash', 'dump']\n",
    "\n",
    "stock_mentions = {}\n",
    "stock_mentions_doll = {}\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def sentiment_extraction(comment):\n",
    "    text = comment.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    bullish_counter = Counter(w for w in text if w in bullish_words)\n",
    "    bearish_counter = Counter(w for w in text if w in bearish_words)\n",
    "    if sum(bearish_counter.values()) > sum(bullish_counter.values()):\n",
    "        return 'bearish'\n",
    "    else:\n",
    "        return 'bullish'\n",
    "\n",
    "\n",
    "for com in all_parsed_comments:\n",
    "    body = ' '.join([word for word in com['body'].split() if word.lower() not in stop_word_list])\n",
    "    com_stocks_names = re.findall(stock_name_re, body)\n",
    "#     com_stocks_names_dol = re.findall(stock_name_re_doll, body)\n",
    "\n",
    "    if len(com_stocks_names) > 0:\n",
    "        for com_stock_name in com_stocks_names:\n",
    "            name = com_stock_name.strip()\n",
    "            if name[0] == '$':\n",
    "                name = name[1:]\n",
    "            if name in stock_mentions:\n",
    "                stock_mentions[name]['count'] += 1\n",
    "                stock_mentions[name]['score'].append(com['comment_score'])\n",
    "                stock_mentions[name]['sentiment'].append(sentiment_extraction(body))\n",
    "            else:\n",
    "                stock_mentions[name] = {\n",
    "                    'count': 1,\n",
    "                    'score': [com['comment_score']],\n",
    "                    'sentiment': [sentiment_extraction(body)]\n",
    "                }\n",
    "\n",
    "\n",
    "for title, thread_data in hot_submissions.items():\n",
    "    com_stocks_names = re.findall(stock_name_re, title)\n",
    "    print(title)\n",
    "    if len(com_stocks_names) > 0:\n",
    "        for com_stock_name in com_stocks_names:\n",
    "            name = com_stock_name.strip()\n",
    "            if name[0] == '$':\n",
    "                name = name[1:]\n",
    "            if name in stock_mentions:\n",
    "                stock_mentions[name]['count'] += 1\n",
    "                stock_mentions[name]['score'].append(thread_data['meta']['score'])\n",
    "                stock_mentions[name]['sentiment'].append(sentiment_extraction(title))\n",
    "            else:\n",
    "                stock_mentions[name] = {}\n",
    "                stock_mentions[name]['count'] = 1\n",
    "                stock_mentions[name]['score'] = [thread_data['meta']['score']]\n",
    "                stock_mentions[name]['sentiment'] = [sentiment_extraction(title)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guessing is not good enough, we import the word corpus to filter out all common words out of our dictionary. We also make sure the count is superior to five as a arbitrary sanity check to make sure this was recorded across multiple comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock name | Count | Score | Sentiment\n",
      "AMC  |  98  |  23265  |  bullish\n",
      "TLRY  |  7  |  164  |  bullish\n",
      "CLOV  |  8  |  170  |  bullish\n",
      "GME  |  17  |  307  |  bullish\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "\n",
    "a = {key:data for key, data in stock_mentions.items() if key.lower() not in word_list and data['count'] > 5}\n",
    "\n",
    "print(\"Stock name | Count | Score | Sentiment\")\n",
    "for key, data in a.items():\n",
    "    print(key,\" | \", data['count'],\" | \", sum(data['score']),\" | \", Counter(data['sentiment']).most_common(1)[0][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in subreddit.hot(limit=1):\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment_queue = submission.comments[:]  # Seed with top-level\n",
    "    while comment_queue:\n",
    "        comment = comment_queue.pop(0)\n",
    "        print(comment.body)\n",
    "        comment_queue.extend(comment.replies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsb",
   "language": "python",
   "name": "wsb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
